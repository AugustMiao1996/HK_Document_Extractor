#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶è¡Œæ–‡æ¡£å¤„ç†å™¨
Parallel Document Processor for Hong Kong Court Documents

Author: AI Assistant
Version: 1.0 - å¹¶è¡Œä¼˜åŒ–ç‰ˆ
"""

import os
import json
import time
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from multiprocessing import Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed
import pandas as pd

# å¯¼å…¥åŸæœ‰çš„æå–å™¨
from extractor import DocumentExtractor

class ParallelBatchProcessor:
    """å¹¶è¡Œæ‰¹é‡æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, output_dir: str = "output", log_dir: str = "logs", max_workers: Optional[int] = None):
        """
        åˆå§‹åŒ–å¹¶è¡Œæ‰¹é‡å¤„ç†å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            log_dir: æ—¥å¿—ç›®å½•
            max_workers: æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°ï¼ŒNoneä¸ºè‡ªåŠ¨æ£€æµ‹
        """
        self.output_dir = Path(output_dir)
        self.log_dir = Path(log_dir)
        
        # è‡ªåŠ¨æ£€æµ‹æœ€ä½³è¿›ç¨‹æ•°
        if max_workers is None:
            cpu_cores = cpu_count()
            if cpu_cores >= 8:
                self.max_workers = min(cpu_cores - 2, 6)  # ç•™2ä¸ªæ ¸å¿ƒï¼Œæœ€å¤š6ä¸ªè¿›ç¨‹
            elif cpu_cores >= 4:
                self.max_workers = min(cpu_cores - 1, 4)  # ç•™1ä¸ªæ ¸å¿ƒ
            else:
                self.max_workers = max(1, cpu_cores // 2)
        else:
            self.max_workers = max_workers
        
        # åˆ›å»ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        self.log_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.logger = self._setup_logger()
        
        self.logger.info(f"å¹¶è¡Œå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨ {self.max_workers} ä¸ªè¿›ç¨‹")
        
    def _setup_logger(self):
        """è®¾ç½®å¤„ç†å™¨æ—¥å¿—"""
        logger = logging.getLogger('ParallelBatchProcessor')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # æ–‡ä»¶æ—¥å¿—
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = self.log_dir / f"parallel_batch_process_{timestamp}.log"
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            
            # æ§åˆ¶å°æ—¥å¿—
            console_handler = logging.StreamHandler()
            
            # æ ¼å¼åŒ–
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            file_handler.setFormatter(formatter)
            console_handler.setFormatter(formatter)
            
            logger.addHandler(file_handler)
            logger.addHandler(console_handler)
        
        return logger
    
    def find_pdf_files(self, input_dir: str, pattern: str = "*.pdf") -> List[str]:
        """æŸ¥æ‰¾PDFæ–‡ä»¶"""
        input_path = Path(input_dir)
        if not input_path.exists():
            self.logger.error(f"Input directory does not exist: {input_dir}")
            return []
        
        pdf_files = list(input_path.glob(pattern))
        self.logger.info(f"Found {len(pdf_files)} PDF files in {input_dir}")
        
        return [str(pdf_file) for pdf_file in pdf_files]
    
    def process_directory_parallel(self, input_dir: str) -> List[Dict[str, str]]:
        """
        å¹¶è¡Œå¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰PDFæ–‡ä»¶
        
        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹å¹¶è¡Œæ‰¹é‡å¤„ç†ç›®å½•: {input_dir}")
        self.logger.info(f"ä½¿ç”¨ {self.max_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹")
        
        # æŸ¥æ‰¾PDFæ–‡ä»¶
        pdf_files = self.find_pdf_files(input_dir)
        if not pdf_files:
            self.logger.warning("No PDF files found")
            return []
        
        # å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
        results = []
        successful = 0
        failed = 0
        
        # åˆ›å»ºè¿›åº¦è·Ÿè¸ª
        total_files = len(pdf_files)
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(process_single_pdf, pdf_file): pdf_file 
                for pdf_file in pdf_files
            }
            
            # æ”¶é›†ç»“æœ
            for i, future in enumerate(as_completed(future_to_file), 1):
                pdf_file = future_to_file[future]
                file_name = Path(pdf_file).name
                
                try:
                    result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                    if result:
                        results.append(result)
                        successful += 1
                        self.logger.info(f"âœ… [{i}/{total_files}] æˆåŠŸå¤„ç†: {file_name}")
                    else:
                        failed += 1
                        self.logger.error(f"âŒ [{i}/{total_files}] å¤„ç†å¤±è´¥: {file_name}")
                except Exception as e:
                    failed += 1
                    self.logger.error(f"âŒ [{i}/{total_files}] å¤„ç†å¼‚å¸¸: {file_name} - {e}")
                
                # æ¯å¤„ç†10ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if i % 10 == 0:
                    elapsed = time.time() - start_time
                    avg_time = elapsed / i
                    remaining = (total_files - i) * avg_time
                    self.logger.info(f"ğŸ“Š è¿›åº¦: {i}/{total_files} ({i/total_files*100:.1f}%), "
                                   f"å·²ç”¨æ—¶: {elapsed:.1f}s, é¢„è®¡å‰©ä½™: {remaining:.1f}s")
        
        # æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        self.logger.info(f"ğŸ‰ å¹¶è¡Œå¤„ç†å®Œæˆ!")
        self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
        self.logger.info(f"ğŸ“Š æˆåŠŸ: {successful}, å¤±è´¥: {failed}")
        self.logger.info(f"âš¡ å¹³å‡é€Ÿåº¦: {total_files/total_time:.1f} æ–‡ä»¶/ç§’")
        
        return results
    
    def generate_summary_report(self, results: List[Dict[str, str]]) -> Dict:
        """
        ç”Ÿæˆå¤„ç†æ‘˜è¦æŠ¥å‘Š - è°ƒç”¨åŸç‰ˆprocessorçš„å®Œæ•´ç»Ÿè®¡åŠŸèƒ½
        
        Args:
            results: å¤„ç†ç»“æœåˆ—è¡¨
            
        Returns:
            æ‘˜è¦æŠ¥å‘Šå­—å…¸
        """
        if not results:
            return {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_files = len(results)
        language_stats = {}
        court_stats = {}
        case_type_stats = {}
        
        # ç»Ÿè®¡å„é¡¹æŒ‡æ ‡
        for result in results:
            # è¯­è¨€ç»Ÿè®¡
            lang = result.get('language', 'unknown')
            language_stats[lang] = language_stats.get(lang, 0) + 1
            
            # æ³•åº­ç»Ÿè®¡
            court = result.get('court_name', 'unknown')
            if court and court != 'unknown':
                court_key = court[:50] + "..." if len(court) > 50 else court
                court_stats[court_key] = court_stats.get(court_key, 0) + 1
            
            # æ¡ˆä»¶ç±»å‹ç»Ÿè®¡
            case_type = result.get('case_type', 'unknown')
            if case_type and case_type != 'unknown':
                # æå–æ¡ˆä»¶ç±»å‹å…³é”®è¯
                if 'application' in case_type.lower():
                    case_type_stats['Application'] = case_type_stats.get('Application', 0) + 1
                elif 'action' in case_type.lower():
                    case_type_stats['Action'] = case_type_stats.get('Action', 0) + 1
                else:
                    case_type_stats['Other'] = case_type_stats.get('Other', 0) + 1
        
        # å­—æ®µå®Œæ•´æ€§ç»Ÿè®¡ - ç»Ÿè®¡æ‰€æœ‰å­—æ®µ
        field_completeness = {}
        
        # ä»ç¬¬ä¸€ä¸ªç»“æœä¸­è·å–æ‰€æœ‰å­—æ®µåï¼ˆé™¤äº†æ–‡ä»¶è·¯å¾„ç›¸å…³å­—æ®µï¼‰
        if results:
            all_fields = [key for key in results[0].keys() if key not in ['file_name', 'file_path']]
        else:
            all_fields = ['trial_date', 'court_name', 'case_number', 'plaintiff', 'defendant', 
                         'judge', 'case_type', 'lawyer', 'judgment_result', 'claim_amount', 
                         'judgment_amount', 'language', 'document_type']
        
        for field in all_fields:
            complete_count = sum(1 for result in results if result.get(field, '').strip())
            field_completeness[field] = {
                'complete': complete_count,
                'missing': total_files - complete_count,
                'percentage': (complete_count / total_files * 100) if total_files > 0 else 0
            }
        
        summary = {
            'processing_time': datetime.now().isoformat(),
            'processing_mode': 'parallel',
            'max_workers': self.max_workers,
            'total_files_processed': total_files,
            'language_distribution': language_stats,
            'court_distribution': court_stats,
            'case_type_distribution': case_type_stats,
            'field_completeness': field_completeness,
            'success_rate': 100.0
        }
        
        # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary_file = self.output_dir / f"parallel_summary_report_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"Summary report saved: {summary_file}")
        return summary

    def save_results(self, results: List[Dict[str, str]], format_type: str = "all") -> Dict[str, str]:
        """ä¿å­˜å¤„ç†ç»“æœ"""
        if not results:
            self.logger.warning("No results to save")
            return {}
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}
        
        # JSONæ ¼å¼
        if format_type in ['json', 'all']:
            json_file = self.output_dir / f"parallel_extraction_results_{timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            saved_files['json'] = str(json_file)
            self.logger.info(f"Results saved to JSON: {json_file}")
        
        # CSVæ ¼å¼
        if format_type in ['csv', 'all']:
            try:
                csv_file = self.output_dir / f"parallel_extraction_results_{timestamp}.csv"
                df = pd.DataFrame(results)
                df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                saved_files['csv'] = str(csv_file)
                self.logger.info(f"Results saved to CSV: {csv_file}")
            except Exception as e:
                self.logger.warning(f"Failed to save CSV: {e}")
        
        # Excelæ ¼å¼
        if format_type in ['excel', 'all']:
            try:
                excel_file = self.output_dir / f"parallel_extraction_results_{timestamp}.xlsx"
                df = pd.DataFrame(results)
                df.to_excel(excel_file, index=False, engine='openpyxl')
                saved_files['excel'] = str(excel_file)
                self.logger.info(f"Results saved to Excel: {excel_file}")
            except Exception as e:
                self.logger.warning(f"Failed to save Excel: {e}")
        
        return saved_files

    def run(self, input_dir: str, output_format: str = "all") -> Dict:
        """
        è¿è¡Œå¹¶è¡Œæ‰¹é‡å¤„ç†
        
        Args:
            input_dir: è¾“å…¥ç›®å½•
            output_format: è¾“å‡ºæ ¼å¼
            
        Returns:
            å¤„ç†ç»“æœæ‘˜è¦
        """
        self.logger.info("=" * 50)
        self.logger.info("å¯åŠ¨é¦™æ¸¯æ³•é™¢æ–‡æ¡£å¹¶è¡Œæå–å™¨")
        self.logger.info(f"å¹¶è¡Œè¿›ç¨‹æ•°: {self.max_workers}")
        self.logger.info("=" * 50)
        
        # å¹¶è¡Œå¤„ç†æ–‡æ¡£
        results = self.process_directory_parallel(input_dir)
        
        if results:
            # ä¿å­˜ç»“æœ
            saved_files = self.save_results(results, output_format)
            
            # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            summary = self.generate_summary_report(results)
            summary['saved_files'] = saved_files
            
            self.logger.info("=" * 50)
            self.logger.info("ğŸ‰ å¹¶è¡Œå¤„ç†æˆåŠŸå®Œæˆ!")
            self.logger.info(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {len(results)}")
            self.logger.info(f"âš¡ ä½¿ç”¨è¿›ç¨‹æ•°: {self.max_workers}")
            self.logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {list(saved_files.values())}")
            self.logger.info("=" * 50)
            
            return summary
        else:
            self.logger.error("æ²¡æœ‰æ–‡ä»¶è¢«æˆåŠŸå¤„ç†")
            return {}


def process_single_pdf(pdf_path: str) -> Optional[Dict[str, str]]:
    """
    å•ä¸ªPDFå¤„ç†å‡½æ•°ï¼ˆä¾›å¤šè¿›ç¨‹è°ƒç”¨ï¼‰
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        
    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    try:
        # æ¯ä¸ªè¿›ç¨‹åˆ›å»ºç‹¬ç«‹çš„æå–å™¨å®ä¾‹
        extractor = DocumentExtractor(log_level=logging.WARNING)  # å‡å°‘æ—¥å¿—è¾“å‡º
        result = extractor.process_pdf(pdf_path)
        return result
    except Exception as e:
        # åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸­ï¼Œå¼‚å¸¸å¤„ç†è¦è°¨æ…
        return None 